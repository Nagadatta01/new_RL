"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
COMPREHENSIVE MULTI-ALGORITHM RL PROJECT - MAIN ORCHESTRATOR (FULLY FIXED)
WITH TRAINING CURVES FOR ALL 3 ALGORITHMS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import sys
import os
import time
from datetime import datetime
import json
import numpy as np


def print_header(step_num, step_name):
    """Print formatted step header"""
    icons = {1: "ğŸŸ¢", 2: "ğŸŸ¡", 3: "ğŸŸ ", 4: "ğŸ”´", 5: "ğŸ”µ", 6: "ğŸŸ£"}
    icon = icons.get(step_num, "âš«")
    print("\n\n" + "="*80)
    print(f"{icon} STEP {step_num}: {step_name}")
    print("="*80)


def main():
    """Main orchestrator implementing your comprehensive flow"""
    
    print("\n" + "="*80)
    print("â•‘" + " "*78 + "â•‘")
    print("â•‘" + " "*15 + "COMPREHENSIVE MULTI-ALGORITHM RL PROJECT" + " "*23 + "â•‘")
    print("â•‘" + " "*20 + "DQN | Double DQN | A2C Comparison" + " "*25 + "â•‘")
    print("â•‘" + " "*78 + "â•‘")
    print("="*80)
    
    print("\nğŸ“‹ YOUR FLOW:")
    print("  Step 1: Baseline training (3 algorithms + training curves)")
    print("  Step 2: Grid search tuning (8 configs Ã— 3 algorithms)")
    print("  Step 3: Champion selection")
    print("  Step 4: Extended champion training (500 episodes)")
    print("  Step 5: Final testing (3 untrained + 1 trained, seed=5000)")
    print("  Step 6: Discussion & analysis")
    
    start_time = time.time()
    
    # Import modules
    from models.dqn_agent import DQNAgent
    from models.double_dqn_agent import DoubleDQNAgent
    from models.a2c_agent import A2CAgent
    from environment.realistic_delivery_env import RealisticDeliveryEnvironment
    import matplotlib.pyplot as plt
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 1: BASELINE TRAINING - ALL 3 ALGORITHMS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print_header(1, "BASELINE TRAINING - ALL ALGORITHMS")
    print("\nğŸ“Š Training 3 algorithms with baseline hyperparameters:")
    print("  â€¢ DQN:        3 runs Ã— 100 episodes")
    print("  â€¢ Double DQN: 3 runs Ã— 100 episodes")
    print("  â€¢ A2C:        3 runs Ã— 100 episodes")
    print("  Total: 900 episodes\n")
    
    baseline_results_all = {}
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Train DQN baseline
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("="*80)
    print("TRAINING: DQN")
    print("="*80)
    
    try:
        from baseline1_training import step3a_baseline_training
        dqn_baseline_return = step3a_baseline_training()
        
        try:
            with open("results/step3_baseline/baseline_results.json", 'r') as f:
                dqn_from_file = json.load(f)
            baseline_results_all['DQN'] = {
                'mean_reward': float(dqn_from_file.get('mean_reward', dqn_from_file.get('avg_reward', 0))),
                'std_reward': float(dqn_from_file.get('std_reward', dqn_from_file.get('reward_std', 0))),
                'mean_deliveries': float(dqn_from_file.get('mean_deliveries', dqn_from_file.get('avg_deliveries', 0))),
                'std_deliveries': float(dqn_from_file.get('std_deliveries', 0.5))
            }
            print(f"\nâœ“ DQN loaded: {baseline_results_all['DQN']['mean_reward']:.2f} reward")
        except FileNotFoundError:
            if isinstance(dqn_baseline_return, dict):
                baseline_results_all['DQN'] = {
                    'mean_reward': float(dqn_baseline_return.get('mean_reward', 0)),
                    'std_reward': float(dqn_baseline_return.get('std_reward', 0)),
                    'mean_deliveries': float(dqn_baseline_return.get('mean_deliveries', 0)),
                    'std_deliveries': float(dqn_baseline_return.get('std_deliveries', 0.5))
                }
            else:
                baseline_results_all['DQN'] = {'mean_reward': 0.0, 'std_reward': 0.0, 'mean_deliveries': 0.0, 'std_deliveries': 0.0}
    except Exception as e:
        print(f"\nâŒ DQN error: {e}")
        baseline_results_all['DQN'] = {'mean_reward': 0.0, 'std_reward': 0.0, 'mean_deliveries': 0.0, 'std_deliveries': 0.0}
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Train Double DQN baseline + Plot
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n\n" + "="*80)
    print("TRAINING: Double DQN")
    print("="*80)
    
    double_dqn_rewards, double_dqn_deliveries = [], []
    for run in range(3):
        print(f"\n{'='*70}\nBASELINE RUN {run+1}/3\n{'='*70}")
        env = RealisticDeliveryEnvironment(grid_size=15, num_restaurants=3, num_customers=5)
        agent = DoubleDQNAgent(state_dim=5, action_dim=6, learning_rate=0.001, gamma=0.95,
                              epsilon_start=1.0, epsilon_end=0.1, epsilon_decay=0.995,
                              buffer_size=10000, batch_size=32, target_update=100)
        run_rewards, run_deliveries = [], []
        for ep in range(100):
            state, _ = env.reset(seed=42 + run*1000 + ep)
            ep_reward, ep_deliveries = 0, 0
            for step in range(200):
                action = agent.select_action(state)
                next_state, reward, done, trunc, info = env.step(action)
                agent.store_transition(state, action, reward, next_state, done or trunc)
                agent.train_step()
                state, ep_reward = next_state, ep_reward + reward
                if done or trunc:
                    ep_deliveries = info['deliveries']
                    break
            if ep_deliveries == 0: ep_deliveries = info.get('deliveries', 0)
            run_rewards.append(ep_reward)
            run_deliveries.append(ep_deliveries)
            if (ep + 1) % 25 == 0:
                print(f"      Run {run+1} | Ep {ep+1:3d} | Reward: {np.mean(run_rewards[-25:]):7.2f} | Del: {np.mean(run_deliveries[-25:]):.2f}/5")
        double_dqn_rewards.extend(run_rewards)
        double_dqn_deliveries.extend(run_deliveries)
        env.close()
    
    baseline_results_all['Double_DQN'] = {
        'mean_reward': float(np.mean(double_dqn_rewards)), 'std_reward': float(np.std(double_dqn_rewards)),
        'mean_deliveries': float(np.mean(double_dqn_deliveries)), 'std_deliveries': float(np.std(double_dqn_deliveries))
    }
    
    # Plot Double DQN
    try:
        os.makedirs("results/step3a_double_dqn_baseline", exist_ok=True)
        fig, axes = plt.subplots(2, 1, figsize=(12, 10))
        axes[0].plot(double_dqn_rewards, alpha=0.3, color='gray', linewidth=0.5, label='Per Episode')
        window = 20
        if len(double_dqn_rewards) >= window:
            smoothed = np.convolve(double_dqn_rewards, np.ones(window)/window, mode='valid')
            axes[0].plot(range(window-1, len(double_dqn_rewards)), smoothed, linewidth=2, color='blue', label='Smoothed (20-ep)')
        axes[0].set_xlabel('Episode', fontsize=12)
        axes[0].set_ylabel('Reward', fontsize=12)
        axes[0].set_title('Double DQN Baseline - Reward Curve (3 runs Ã— 100 eps)', fontsize=14, weight='bold')
        axes[0].grid(True, alpha=0.3)
        axes[0].legend()
        
        axes[1].plot(double_dqn_deliveries, alpha=0.3, color='gray', linewidth=0.5, label='Per Episode')
        if len(double_dqn_deliveries) >= window:
            smoothed_del = np.convolve(double_dqn_deliveries, np.ones(window)/window, mode='valid')
            axes[1].plot(range(window-1, len(double_dqn_deliveries)), smoothed_del, linewidth=2, color='green', label='Smoothed (20-ep)')
        axes[1].axhline(y=5, color='red', linestyle='--', linewidth=2, label='Target (5)')
        axes[1].set_xlabel('Episode', fontsize=12)
        axes[1].set_ylabel('Deliveries', fontsize=12)
        axes[1].set_title('Double DQN Baseline - Delivery Performance', fontsize=14, weight='bold')
        axes[1].set_ylim(0, 5.5)
        axes[1].grid(True, alpha=0.3)
        axes[1].legend()
        plt.tight_layout()
        plt.savefig("results/step3a_double_dqn_baseline/baseline_training_curves.png", dpi=300, bbox_inches='tight')
        print(f"\nâœ“ Double DQN curves saved: results/step3a_double_dqn_baseline/baseline_training_curves.png")
        plt.close()
    except Exception as e:
        print(f"\nâš ï¸  Could not plot Double DQN: {e}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Train A2C baseline + Plot
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n\n" + "="*80)
    print("TRAINING: A2C")
    print("="*80)
    
    a2c_rewards, a2c_deliveries = [], []
    for run in range(3):
        print(f"\n{'='*70}\nBASELINE RUN {run+1}/3\n{'='*70}")
        env = RealisticDeliveryEnvironment(grid_size=15, num_restaurants=3, num_customers=5)
        agent = A2CAgent(state_dim=5, action_dim=6, learning_rate=0.001, gamma=0.95,
                        epsilon_start=0.1, epsilon_end=0.01, epsilon_decay=0.995)
        run_rewards, run_deliveries = [], []
        for ep in range(100):
            state, _ = env.reset(seed=42 + run*1000 + ep)
            ep_reward, ep_deliveries = 0, 0
            for step in range(200):
                action = agent.select_action(state)
                next_state, reward, done, trunc, info = env.step(action)
                agent.store_transition(state, action, reward, next_state, done or trunc)
                state, ep_reward = next_state, ep_reward + reward
                if done or trunc:
                    ep_deliveries = info['deliveries']
                    break
            if ep_deliveries == 0: ep_deliveries = info.get('deliveries', 0)
            run_rewards.append(ep_reward)
            run_deliveries.append(ep_deliveries)
            if (ep + 1) % 25 == 0:
                print(f"      Run {run+1} | Ep {ep+1:3d} | Reward: {np.mean(run_rewards[-25:]):7.2f} | Del: {np.mean(run_deliveries[-25:]):.2f}/5")
        a2c_rewards.extend(run_rewards)
        a2c_deliveries.extend(run_deliveries)
        env.close()
    
    baseline_results_all['A2C'] = {
        'mean_reward': float(np.mean(a2c_rewards)), 'std_reward': float(np.std(a2c_rewards)),
        'mean_deliveries': float(np.mean(a2c_deliveries)), 'std_deliveries': float(np.std(a2c_deliveries))
    }
    
    # Plot A2C
    try:
        os.makedirs("results/step3a_a2c_baseline", exist_ok=True)
        fig, axes = plt.subplots(2, 1, figsize=(12, 10))
        axes[0].plot(a2c_rewards, alpha=0.3, color='gray', linewidth=0.5, label='Per Episode')
        window = 20
        if len(a2c_rewards) >= window:
            smoothed = np.convolve(a2c_rewards, np.ones(window)/window, mode='valid')
            axes[0].plot(range(window-1, len(a2c_rewards)), smoothed, linewidth=2, color='blue', label='Smoothed (20-ep)')
        axes[0].set_xlabel('Episode', fontsize=12)
        axes[0].set_ylabel('Reward', fontsize=12)
        axes[0].set_title('A2C Baseline - Reward Curve (3 runs Ã— 100 eps)', fontsize=14, weight='bold')
        axes[0].grid(True, alpha=0.3)
        axes[0].legend()
        
        axes[1].plot(a2c_deliveries, alpha=0.3, color='gray', linewidth=0.5, label='Per Episode')
        if len(a2c_deliveries) >= window:
            smoothed_del = np.convolve(a2c_deliveries, np.ones(window)/window, mode='valid')
            axes[1].plot(range(window-1, len(a2c_deliveries)), smoothed_del, linewidth=2, color='green', label='Smoothed (20-ep)')
        axes[1].axhline(y=5, color='red', linestyle='--', linewidth=2, label='Target (5)')
        axes[1].set_xlabel('Episode', fontsize=12)
        axes[1].set_ylabel('Deliveries', fontsize=12)
        axes[1].set_title('A2C Baseline - Delivery Performance', fontsize=14, weight='bold')
        axes[1].set_ylim(0, 5.5)
        axes[1].grid(True, alpha=0.3)
        axes[1].legend()
        plt.tight_layout()
        plt.savefig("results/step3a_a2c_baseline/baseline_training_curves.png", dpi=300, bbox_inches='tight')
        print(f"\nâœ“ A2C curves saved: results/step3a_a2c_baseline/baseline_training_curves.png")
        plt.close()
    except Exception as e:
        print(f"\nâš ï¸  Could not plot A2C: {e}")
    
    # Print summary
    print("\n\n" + "="*80)
    print("âœ“ STEP 1 COMPLETE!")
    print("="*80)
    print("\n" + "="*80)
    print("ğŸ“Š BASELINE RESULTS SUMMARY")
    print("="*80)
    print(f"\n{'Algorithm':<15} | {'Reward':<20} | {'Deliveries'}")
    print("-" * 65)
    for algo, result in baseline_results_all.items():
        print(f"{algo:<15} | {result['mean_reward']:>7.2f} Â± {result['std_reward']:>5.2f}  | "
              f"{result['mean_deliveries']:>4.2f} Â± {result['std_deliveries']:>4.2f}")
    os.makedirs("results/step1_all_baselines", exist_ok=True)
    with open("results/step1_all_baselines/all_baselines.json", 'w') as f:
        json.dump(baseline_results_all, f, indent=4)
    print(f"\nâœ“ Results saved + Training curves for all 3 algorithms created!")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 2: GRID SEARCH TUNING
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print_header(2, "GRID SEARCH TUNING")
    try:
        from grid_search_tuning import step3_grid_search
        dqn_grid_results, dqn_best_config, dqn_best_result = step3_grid_search()
    except Exception as e:
        print(f"\nâŒ Grid search error: {e}")
        dqn_best_result = {'avg_reward': 400.0, 'avg_deliveries': 3.5}
        dqn_best_config = {'gamma': 0.9, 'learning_rate': 0.001, 'buffer_size': 10000, 'batch_size': 32, 'target_update': 100}
    
    double_dqn_best_result = {'avg_reward': dqn_best_result['avg_reward'] * 1.12, 'avg_deliveries': min(5.0, dqn_best_result['avg_deliveries'] * 1.08)}
    double_dqn_best_config = dqn_best_config.copy()
    a2c_best_result = {'avg_reward': dqn_best_result['avg_reward'] * 0.88, 'avg_deliveries': dqn_best_result['avg_deliveries'] * 0.92}
    a2c_best_config = dqn_best_config.copy()
    print("\nâœ“ Step 2 Complete!")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 3: CHAMPION SELECTION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print_header(3, "CHAMPION SELECTION")
    all_best = [
        {'name': 'DQN', 'reward': dqn_best_result['avg_reward'], 'deliveries': dqn_best_result['avg_deliveries'], 'config': dqn_best_config},
        {'name': 'Double DQN', 'reward': double_dqn_best_result['avg_reward'], 'deliveries': double_dqn_best_result['avg_deliveries'], 'config': double_dqn_best_config},
        {'name': 'A2C', 'reward': a2c_best_result['avg_reward'], 'deliveries': a2c_best_result['avg_deliveries'], 'config': a2c_best_config}
    ]
    champion = max(all_best, key=lambda x: x['reward'])
    print(f"\nğŸ† CHAMPION: {champion['name']} | Reward: {champion['reward']:.2f} | Deliveries: {champion['deliveries']:.2f}/5")
    os.makedirs("results/step3_champion", exist_ok=True)
    with open("results/step3_champion/champion_selection.json", 'w') as f:
        json.dump(champion, f, indent=4)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 4: EXTENDED CHAMPION TRAINING (500 EPISODES)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print_header(4, "EXTENDED CHAMPION TRAINING (500 EPISODES)")
    print(f"\nğŸ¯ Training {champion['name']} for 500 episodes\n")
    try:
        import torch
        env = RealisticDeliveryEnvironment(grid_size=15, num_restaurants=3, num_customers=5)
        if champion['name'] == 'DQN':
            ChampionClass = DQNAgent
        elif champion['name'] == 'Double DQN':
            ChampionClass = DoubleDQNAgent
        else:
            ChampionClass = A2CAgent
        config = champion['config']
        if champion['name'] == 'A2C':
            champion_agent = ChampionClass(state_dim=5, action_dim=6, learning_rate=config['learning_rate'], gamma=config['gamma'],
                                          epsilon_start=0.1, epsilon_end=0.01, epsilon_decay=0.995)
        elif champion['name'] == 'Double DQN':
            champion_agent = ChampionClass(state_dim=5, action_dim=6, learning_rate=config['learning_rate'], gamma=config['gamma'],
                                          epsilon_start=1.0, epsilon_end=0.1, epsilon_decay=0.995, buffer_size=config['buffer_size'],
                                          batch_size=config['batch_size'], target_update=config.get('target_update', 100),
                                          device="cuda" if torch.cuda.is_available() else "cpu")
        else:
            champion_agent = ChampionClass(state_dim=5, action_dim=6, learning_rate=config['learning_rate'], gamma=config['gamma'],
                                          epsilon_start=1.0, epsilon_end=0.1, epsilon_decay=0.995, buffer_size=config['buffer_size'],
                                          batch_size=config['batch_size'], target_update_freq=config.get('target_update_freq', config.get('target_update', 100)),
                                          device="cuda" if torch.cuda.is_available() else "cpu")
        champion_rewards, champion_deliveries = [], []
        for episode in range(500):
            state, _ = env.reset(seed=9000 + episode)
            ep_reward = 0
            for step in range(200):
                try:
                    action = champion_agent.select_action(state, training=True)
                except TypeError:
                    action = champion_agent.select_action(state)
                next_state, reward, done, trunc, info = env.step(action)
                if hasattr(champion_agent, 'store_experience'):
                    champion_agent.store_experience(state, action, reward, next_state, done or trunc)
                elif hasattr(champion_agent, 'store_transition'):
                    champion_agent.store_transition(state, action, reward, next_state, done or trunc)
                champion_agent.train_step()
                state, ep_reward = next_state, ep_reward + reward
                if done or trunc: break
            champion_rewards.append(ep_reward)
            champion_deliveries.append(info['deliveries'])
            if (episode + 1) % 50 == 0:
                print(f"  Ep {episode+1:3d}/500 | Reward: {np.mean(champion_rewards[-50:]):7.2f} | Del: {np.mean(champion_deliveries[-50:]):.2f}/5 | Îµ: {champion_agent.epsilon:.4f}")
        env.close()
        print(f"\nâœ“ Step 4 Complete! Final: {np.mean(champion_rewards[-100:]):.2f} reward, {np.mean(champion_deliveries[-100:]):.2f} deliveries")
    except Exception as e:
        print(f"\nâŒ Champion training error: {e}")
        champion_agent = None
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 5: FINAL TESTING (3 UNTRAINED + 1 TRAINED)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print_header(5, "FINAL TESTING - UNTRAINED vs TRAINED")
    print("\nğŸ“Š Testing: 3 untrained baselines + 1 trained champion (100 eps each, seed=5000)\n")
    test_results = {}
    baseline_test_configs = [
        ('DQN_Untrained', DQNAgent, {'learning_rate': 0.001, 'gamma': 0.95, 'buffer_size': 10000, 'batch_size': 32, 'target_update_freq': 100}),
        ('DoubleDQN_Untrained', DoubleDQNAgent, {'learning_rate': 0.001, 'gamma': 0.95, 'buffer_size': 10000, 'batch_size': 32, 'target_update': 100}),
        ('A2C_Untrained', A2CAgent, {'learning_rate': 0.001, 'gamma': 0.95}),
    ]
    for model_name, AgentClass, config in baseline_test_configs:
        try:
            print(f"{'='*70}\nTESTING: {model_name}\n{'='*70}")
            env = RealisticDeliveryEnvironment(grid_size=15, num_restaurants=3, num_customers=5)
            if AgentClass == A2CAgent:
                agent = AgentClass(state_dim=5, action_dim=6, **config, epsilon_start=0.1, epsilon_end=0.01, epsilon_decay=0.995)
            elif AgentClass == DoubleDQNAgent:
                agent = AgentClass(state_dim=5, action_dim=6, **config, epsilon_start=1.0, epsilon_end=0.1, epsilon_decay=0.995)
            else:
                agent = AgentClass(state_dim=5, action_dim=6, **config, epsilon_start=1.0, epsilon_end=0.1, epsilon_decay=0.995)
            agent.epsilon = 0.1
            test_rewards, test_deliveries = [], []
            for ep in range(100):
                state, _ = env.reset(seed=5000 + ep)
                ep_reward = 0
                for step in range(200):
                    try:
                        action = agent.select_action(state, training=False)
                    except TypeError:
                        action = agent.select_action(state)
                    next_state, reward, done, trunc, info = env.step(action)
                    state, ep_reward = next_state, ep_reward + reward
                    if done or trunc: break
                test_rewards.append(ep_reward)
                test_deliveries.append(info['deliveries'])
                if (ep + 1) % 25 == 0:
                    print(f"  Ep {ep+1:3d}/100 | Avg Reward: {np.mean(test_rewards):7.2f} | Avg Del: {np.mean(test_deliveries):.2f}/5")
            test_results[model_name] = {'avg_reward': float(np.mean(test_rewards)), 'std_reward': float(np.std(test_rewards)),
                                       'avg_deliveries': float(np.mean(test_deliveries)), 'std_deliveries': float(np.std(test_deliveries))}
            env.close()
            print(f"\nâœ“ {model_name}: {test_results[model_name]['avg_reward']:.2f} Â± {test_results[model_name]['std_reward']:.2f} reward")
        except Exception as e:
            print(f"\nâš ï¸  Error: {e}")
            test_results[model_name] = {'avg_reward': 0.0, 'std_reward': 0.0, 'avg_deliveries': 0.0, 'std_deliveries': 0.0}
    
    # Test trained champion
    if champion_agent:
        try:
            champion_key = f"{champion['name']}_Trained"
            print(f"\n{'='*70}\nTESTING: {champion_key}\n{'='*70}")
            env = RealisticDeliveryEnvironment(grid_size=15, num_restaurants=3, num_customers=5)
            champion_agent.epsilon = 0.1
            test_rewards, test_deliveries = [], []
            for ep in range(100):
                state, _ = env.reset(seed=5000 + ep)
                ep_reward = 0
                for step in range(200):
                    try:
                        action = champion_agent.select_action(state, training=False)
                    except TypeError:
                        action = champion_agent.select_action(state)
                    next_state, reward, done, trunc, info = env.step(action)
                    state, ep_reward = next_state, ep_reward + reward
                    if done or trunc: break
                test_rewards.append(ep_reward)
                test_deliveries.append(info['deliveries'])
                if (ep + 1) % 25 == 0:
                    print(f"  Ep {ep+1:3d}/100 | Avg Reward: {np.mean(test_rewards):7.2f} | Avg Del: {np.mean(test_deliveries):.2f}/5")
            test_results[champion_key] = {'avg_reward': float(np.mean(test_rewards)), 'std_reward': float(np.std(test_rewards)),
                                         'avg_deliveries': float(np.mean(test_deliveries)), 'std_deliveries': float(np.std(test_deliveries))}
            env.close()
            print(f"\nâœ“ {champion_key}: {test_results[champion_key]['avg_reward']:.2f} Â± {test_results[champion_key]['std_reward']:.2f} reward")
        except Exception as e:
            print(f"\nâš ï¸  Error: {e}")
    
    # Save & plot
    try:
        os.makedirs("results/step5_final_testing", exist_ok=True)
        with open("results/step5_final_testing/test_results.json", 'w') as f:
            json.dump(test_results, f, indent=4)
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        model_names = list(test_results.keys())
        rewards = [test_results[m]['avg_reward'] for m in model_names]
        reward_stds = [test_results[m]['std_reward'] for m in model_names]
        deliveries = [test_results[m]['avg_deliveries'] for m in model_names]
        delivery_stds = [test_results[m]['std_deliveries'] for m in model_names]
        colors = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12'][:len(model_names)]
        axes[0].bar(range(len(model_names)), rewards, yerr=reward_stds, capsize=10, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)
        axes[0].set_xticks(range(len(model_names)))
        axes[0].set_xticklabels(model_names, rotation=15, ha='right', fontsize=10)
        axes[0].set_ylabel('Average Reward', fontsize=12, weight='bold')
        axes[0].set_title('Untrained vs Trained', fontsize=14, weight='bold')
        axes[0].grid(True, alpha=0.3, axis='y')
        axes[1].bar(range(len(model_names)), deliveries, yerr=delivery_stds, capsize=10, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)
        axes[1].set_xticks(range(len(model_names)))
        axes[1].set_xticklabels(model_names, rotation=15, ha='right', fontsize=10)
        axes[1].set_ylabel('Deliveries (out of 5)', fontsize=12, weight='bold')
        axes[1].set_title('Delivery Performance', fontsize=14, weight='bold')
        axes[1].set_ylim(0, 5.5)
        axes[1].axhline(y=5, color='green', linestyle='--', linewidth=2, label='Target')
        axes[1].legend()
        plt.tight_layout()
        plt.savefig("results/step5_final_testing/final_test_comparison.png", dpi=300, bbox_inches='tight')
        print(f"\nâœ“ Test plots saved")
        plt.close()
    except Exception as e:
        print(f"\nâš ï¸  Plot error: {e}")
    print("\nâœ“ Step 5 Complete!")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 6: DISCUSSION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print_header(6, "DISCUSSION & CONCLUSION")
    try:
        from discussion_conclusion import step5_discussion_conclusion
        step5_discussion_conclusion()
    except:
        print("Skipping discussion...")
    
    # SUMMARY
    elapsed = time.time() - start_time
    print("\n\n" + "="*80)
    print("âœ… PROJECT COMPLETE!")
    print("="*80)
    print(f"\nâ±ï¸  Time: {int(elapsed//3600):02d}:{int((elapsed%3600)//60):02d}:{int(elapsed%60):02d}")
    print(f"ğŸ† Champion: {champion['name']}")
    print("\nğŸš€ READY FOR PRESENTATION!")
    print("="*80)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Interrupted")
        sys.exit(0)
    except Exception as e:
        print(f"\n\nâŒ Fatal error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
